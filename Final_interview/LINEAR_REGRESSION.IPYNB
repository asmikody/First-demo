{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "25a88b67",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'data' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# Regression\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[43mdata\u001b[49m\u001b[38;5;241m.\u001b[39mdrop([\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mLife expectancy \u001b[39m\u001b[38;5;124m\"\u001b[39m,\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCountry\u001b[39m\u001b[38;5;124m\"\u001b[39m],axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m      3\u001b[0m y \u001b[38;5;241m=\u001b[39m data[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mLife expectancy \u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[0;32m      5\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmodel_selection\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m train_test_split\n",
      "\u001b[1;31mNameError\u001b[0m: name 'data' is not defined"
     ]
    }
   ],
   "source": [
    "# Regression\n",
    "x = data.drop([\"Life expectancy \",\"Country\"],axis=1)\n",
    "y = data[\"Life expectancy \"]\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(x, y, test_size = 0.2, random_state = 42)\n",
    "\n",
    "from sklearn.linear_model import LinearRegression\n",
    "lr_model = LinearRegression()\n",
    "lr_model.fit(X_train, y_train)\n",
    "\n",
    "y_predict_lr_train = lr_model.predict(X_train)\n",
    "y_predict_lr_test = lr_model.predict(X_test)\n",
    "\n",
    "from sklearn.metrics import r2_score \n",
    "print(r2_score(y_train, y_predict_lr_train))\n",
    "print(\"\\n\")\n",
    "print(r2_score(y_test, y_predict_lr_test))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09d0bba5",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
    "\n",
    "# -----------------------------\n",
    "# 1. Load dataset\n",
    "# -----------------------------\n",
    "df = pd.read_csv(\"food_delivery_data.csv\")\n",
    "\n",
    "# Drop Order_ID (not useful for prediction)\n",
    "df = df.drop(columns=[\"Order_ID\"])\n",
    "\n",
    "# -----------------------------\n",
    "# 2. One-Hot Encode categorical variables\n",
    "# -----------------------------\n",
    "categorical_cols = [\"Weather\", \"Traffic_Level\", \"Time_of_Day\", \"Vehicle_Type\"]\n",
    "\n",
    "df = pd.get_dummies(df, columns=categorical_cols, drop_first=True)\n",
    "\n",
    "# -----------------------------\n",
    "# 3. Features (X) & Target (y)\n",
    "# -----------------------------\n",
    "X = df.drop(\"Delivery_Time_min\", axis=1)\n",
    "y = df[\"Delivery_Time_min\"]\n",
    "\n",
    "# -----------------------------\n",
    "# 4. Train-Test Split\n",
    "# -----------------------------\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "# -----------------------------\n",
    "# 5. Train Model\n",
    "# -----------------------------\n",
    "model = LinearRegression()\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# -----------------------------\n",
    "# 6. Predictions & Evaluation\n",
    "# -----------------------------\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "print(\"âœ… Model Evaluation:\")\n",
    "print(\"Mean Absolute Error (MAE):\", mean_absolute_error(y_test, y_pred))\n",
    "print(\"Root Mean Squared Error (RMSE):\", np.sqrt(mean_squared_error(y_test, y_pred)))\n",
    "print(\"RÂ² Score:\", r2_score(y_test, y_pred))\n",
    "\n",
    "# -----------------------------\n",
    "# 7. View Coefficients (Feature Importance)\n",
    "# -----------------------------\n",
    "coefficients = pd.DataFrame({\n",
    "    \"Feature\": X.columns,\n",
    "    \"Coefficient\": model.coef_\n",
    "}).sort_values(by=\"Coefficient\", ascending=False)\n",
    "\n",
    "print(\"\\nðŸ“Š Feature Importance (Coefficients):\")\n",
    "print(coefficients)\n",
    "\n",
    "    ]\n",
    ")\n",
    "\n",
    "# -----------------------------\n",
    "# 4. Build pipeline (Preprocessing + Model)\n",
    "# -----------------------------\n",
    "model = Pipeline(steps=[\n",
    "    (\"preprocessor\", preprocessor),\n",
    "    (\"regressor\", LinearRegression())\n",
    "])\n",
    "\n",
    "# -----------------------------\n",
    "# 5. Train-Test Split\n",
    "# -----------------------------\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# -----------------------------\n",
    "# 6. Train Model\n",
    "# -----------------------------\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# -----------------------------\n",
    "# 7. Predictions & Evaluation\n",
    "# -----------------------------\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "print(\"âœ… Model Evaluation:\")\n",
    "print(\"Mean Absolute Error (MAE):\", mean_absolute_error(y_test, y_pred))\n",
    "print(\"Root Mean Squared Error (RMSE):\", np.sqrt(mean_squared_error(y_test, y_pred)))\n",
    "print(\"RÂ² Score:\", r2_score(y_test, y_pred))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "693811df",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Performance:\n",
      "LinearRegression: MAE=5.32, RMSE=8.27, RÂ²=0.833\n",
      "DecisionTree: MAE=9.97, RMSE=13.91, RÂ²=0.527\n",
      "RandomForest: MAE=6.98, RMSE=9.93, RÂ²=0.759\n",
      "XGBoost: MAE=7.13, RMSE=10.16, RÂ²=0.747\n",
      "\n",
      "Top Features from XGBoost:\n",
      "Distance_km               0.322315\n",
      "Preparation_Time_min      0.118950\n",
      "Weather_Foggy             0.080057\n",
      "Traffic_Level_Low         0.078130\n",
      "Weather_Snowy             0.065797\n",
      "Vehicle_Type_Car          0.052016\n",
      "Traffic_Level_Medium      0.048813\n",
      "Weather_Rainy             0.045349\n",
      "Time_of_Day_Evening       0.044973\n",
      "Courier_Experience_yrs    0.039262\n",
      "dtype: float32\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from xgboost import XGBRegressor\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
    "\n",
    "# Step 1: Load dataset\n",
    "df = pd.read_csv(r\"C:\\Users\\BAPS\\Downloads\\archive (12)\\Food_Delivery_Times.csv\")\n",
    "df = df.drop(columns=[\"Order_ID\"], errors='ignore')\n",
    "\n",
    "# Handle missing values\n",
    "df = df.dropna()\n",
    "\n",
    "# One-hot encode categorical features\n",
    "categorical = [\"Weather\", \"Traffic_Level\", \"Time_of_Day\", \"Vehicle_Type\"]\n",
    "df = pd.get_dummies(df, columns=categorical, drop_first=True,dtype=True)\n",
    "\n",
    "# Step 2: Features & target\n",
    "X = df.drop(\"Delivery_Time_min\", axis=1)\n",
    "y = df[\"Delivery_Time_min\"]\n",
    "\n",
    "# Split\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "# Step 3: Define models\n",
    "models = {\n",
    "    \"LinearRegression\": LinearRegression(),\n",
    "    \"DecisionTree\": DecisionTreeRegressor(random_state=42),\n",
    "    \"RandomForest\": RandomForestRegressor(random_state=42, n_estimators=100),\n",
    "    \"XGBoost\": XGBRegressor(random_state=42, n_estimators=100, learning_rate=0.1)\n",
    "}\n",
    "\n",
    "# Train and evaluate\n",
    "results = {}\n",
    "for name, model in models.items():\n",
    "    model.fit(X_train, y_train)\n",
    "    pred = model.predict(X_test)\n",
    "    results[name] = {\n",
    "        \"MAE\": mean_absolute_error(y_test, pred),\n",
    "        \"RMSE\": np.sqrt(mean_squared_error(y_test, pred)),\n",
    "        \"R2\": r2_score(y_test, pred)\n",
    "    }\n",
    "\n",
    "# Display results\n",
    "print(\"Model Performance:\")\n",
    "for name, metrics in results.items():\n",
    "    print(f\"{name}: MAE={metrics['MAE']:.2f}, RMSE={metrics['RMSE']:.2f}, RÂ²={metrics['R2']:.3f}\")\n",
    "\n",
    "# Feature importance (XGBoost)\n",
    "xgb = models[\"XGBoost\"]\n",
    "importances = pd.Series(xgb.feature_importances_, index=X.columns).sort_values(ascending=False)\n",
    "print(\"\\nTop Features from XGBoost:\")\n",
    "print(importances.head(10))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85b963b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import Ridge\n",
    "from sklearn.metrics import r2_score\n",
    "\n",
    "# Create Ridge model with a fixed alpha\n",
    "ridge_model = Ridge(alpha=1.0)\n",
    "\n",
    "# Fit the model\n",
    "ridge_model.fit(X_train, y_train)\n",
    "\n",
    "# Predict\n",
    "y_pred_train = ridge_model.predict(X_train)\n",
    "y_pred_test = ridge_model.predict(X_test)\n",
    "\n",
    "# Evaluate\n",
    "print(\"Train RÂ² score:\", r2_score(y_train, y_pred_train))\n",
    "print(\"Test RÂ² score:\", r2_score(y_test, y_pred_test))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7532e71a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import Ridge\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import r2_score\n",
    "\n",
    "# Define Ridge model\n",
    "ridge = Ridge()\n",
    "\n",
    "ridge = Ridge(alpha=1.0, solver=\"auto\")\n",
    "\n",
    "# Define grid of hyperparameters\n",
    "param_grid = {\n",
    "    \"alpha\": [0.01, 0.1, 1, 10, 100],   # try different regularization strengths\n",
    "    \"fit_intercept\": [True, False]      # whether to include intercept\n",
    "}\n",
    "param_grid = {\n",
    "    \"alpha\": [0.01, 0.1, 1, 10, 100],\n",
    "    \"fit_intercept\": [True, False],\n",
    "    \"solver\": [\"auto\", \"svd\", \"cholesky\", \"lsqr\", \"sparse_cg\", \"saga\"],\n",
    "    \"max_iter\": [1000, 5000, 10000],\n",
    "    \"tol\": [1e-3, 1e-4, 1e-5]\n",
    "}\n",
    "\n",
    "\n",
    "# Grid search with 5-fold cross validation\n",
    "grid = GridSearchCV(\n",
    "    ridge, \n",
    "    param_grid, \n",
    "    cv=5,              # 5-fold cross validation\n",
    "    scoring='r2',      # performance metric\n",
    "    n_jobs=-1\n",
    ")\n",
    "\n",
    "grid.fit(X_train, y_train)\n",
    "\n",
    "print(\"Best Parameters:\", grid.best_params_)\n",
    "print(\"Best Cross-Validation Score:\", grid.best_score_)\n",
    "\n",
    "# Evaluate on test data\n",
    "y_pred = grid.pcision\n",
    "redict(X_test)\n",
    "print(\"Test RÂ²:\", r2_score(y_test, y_pred))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7150c1f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import Lasso\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "lasso = Lasso()\n",
    "\n",
    "param_grid = {\n",
    "    \"alpha\": [0.0001, 0.001, 0.01, 0.1, 1, 10],\n",
    "    \"max_iter\": [1000, 5000, 10000],\n",
    "    \"tol\": [1e-2, 1e-3, 1e-4],\n",
    "    \"fit_intercept\": [True, False],\n",
    "    \"selection\": [\"cyclic\", \"random\"]\n",
    "}\n",
    "\n",
    "grid = GridSearchCV(\n",
    "    estimator=lasso,\n",
    "    param_grid=param_grid,\n",
    "    cv=5,\n",
    "    scoring=\"r2\",\n",
    "    n_jobs=-1\n",
    ")\n",
    "\n",
    "grid.fit(X_train, y_train)\n",
    "\n",
    "print(\"Best Parameters:\", grid.best_params_)\n",
    "print(\"Best CV RÂ²:\", grid.best_score_)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30158f78",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.metrics import r2_score\n",
    "\n",
    "# Define Decision Tree with defaults\n",
    "dt = DecisionTreeRegressor(random_state=42)\n",
    "\n",
    "# Fit\n",
    "dt.fit(X_train, y_train)\n",
    "\n",
    "# Predict\n",
    "y_pred = dt.predict(X_test)\n",
    "\n",
    "# Evaluate\n",
    "print(\"Test RÂ²:\", r2_score(y_test, y_pred))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f989fea",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.model_selection import GridSearchCV\n",
    " \n",
    "# Define model\n",
    "dt = DecisionTreeRegressor(random_state=42)\n",
    " \n",
    "# Define hyperparameter grid\n",
    "param_grid = {\n",
    "    'max_depth': [3, 5, 10, 15, 20, None],\n",
    "    'min_samples_split': list(range(2, 10)),\n",
    "    'min_samples_leaf': list(range(1, 5)),\n",
    "    'max_features': ['sqrt', 'log2', None],\n",
    "    'criterion': ['squared_error', 'friedman_mse']\n",
    "}\n",
    " \n",
    "# GridSearchCV\n",
    "grid_search = GridSearchCV(\n",
    "    estimator=dt,\n",
    "    param_grid=param_grid,\n",
    "    cv=10,\n",
    "    scoring='r2',\n",
    "    n_jobs=-1\n",
    ")\n",
    " \n",
    "# Fit model\n",
    "grid_search.fit(X_train, y_train)\n",
    " \n",
    "# Best model\n",
    "best_dt = grid_search.best_estimator_\n",
    " \n",
    "# Predict\n",
    "y_predict_train = best_dt.predict(X_train)\n",
    "y_predict_test = best_dt.predict(X_test)\n",
    " \n",
    "# Scores\n",
    "from sklearn.metrics import r2_score\n",
    "print(\"Train R2:\", r2_score(y_train, y_predict_train))\n",
    "print(\"Test R2 :\", r2_score(y_test, y_predict_test))\n",
    "print(\"Best Parameters:\", grid_search.best_params_)\n",
    " \n",
    "\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00653265",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.metrics import r2_score\n",
    "\n",
    "# Define model (default params)\n",
    "rf_model = RandomForestRegressor(random_state=42)\n",
    "\n",
    "# Train\n",
    "rf_model.fit(X_train, y_train)\n",
    "\n",
    "# Predictions\n",
    "y_predict_rf_train = rf_model.predict(X_train)\n",
    "y_predict_rf_test = rf_model.predict(X_test)\n",
    "\n",
    "# RÂ² Scores\n",
    "print(\"Train RÂ²:\", r2_score(y_train, y_predict_rf_train))\n",
    "print(\"Test RÂ²:\", r2_score(y_test, y_predict_rf_test))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "151ddbae",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "# Base model\n",
    "rf = RandomForestRegressor(random_state=42)\n",
    "\n",
    "# Parameter grid\n",
    "param_grid = {\n",
    "    \"n_estimators\": [100, 200, 500],\n",
    "    \"max_depth\": [5, 10, 20, None],\n",
    "    \"min_samples_split\": [2, 5, 10],\n",
    "    \"min_samples_leaf\": [1, 2, 4],\n",
    "    \"max_features\": [\"sqrt\", \"log2\", None],\n",
    "    \"bootstrap\": [True, False]\n",
    "}\n",
    "\n",
    "# Grid Search\n",
    "grid_search = GridSearchCV(\n",
    "    estimator=rf,\n",
    "    param_grid=param_grid,\n",
    "    cv=5,\n",
    "    scoring=\"r2\",\n",
    "    n_jobs=-1,\n",
    "    verbose=2\n",
    ")\n",
    "\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "print(\"Best Parameters:\", grid_search.best_params_)\n",
    "print(\"Best CV Score:\", grid_search.best_score_)\n",
    "\n",
    "# Evaluate on test set\n",
    "best_rf = grid_search.best_estimator_\n",
    "y_pred = best_rf.predict(X_test)\n",
    "print(\"Test RÂ²:\", r2_score(y_test, y_pred))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a26714ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "import numpy as np\n",
    "\n",
    "rf = RandomForestRegressor(random_state=42)\n",
    "\n",
    "param_dist = {\n",
    "    \"n_estimators\": [100, 200, 500, 1000],\n",
    "    \"max_depth\": [5, 10, 20, None],\n",
    "    \"min_samples_split\": [2, 5, 10, 20],\n",
    "    \"min_samples_leaf\": [1, 2, 4, 10],\n",
    "    \"max_features\": [\"sqrt\", \"log2\", None],\n",
    "    \"bootstrap\": [True, False]\n",
    "}\n",
    "\n",
    "random_search = RandomizedSearchCV(\n",
    "    estimator=rf,\n",
    "    param_distributions=param_dist,\n",
    "    n_iter=30,   # try 30 random combinations\n",
    "    cv=5,\n",
    "    scoring=\"r2\",\n",
    "    n_jobs=-1,\n",
    "    random_state=42,\n",
    "    verbose=2\n",
    ")\n",
    "\n",
    "random_search.fit(X_train, y_train)\n",
    "\n",
    "print(\"Best Parameters:\", random_search.best_params_)\n",
    "print(\"Best CV Score:\", random_search.best_score_)\n",
    "\n",
    "# Test set evaluation\n",
    "best_rf = random_search.best_estimator_\n",
    "y_pred = best_rf.predict(X_test)\n",
    "print(\"Test RÂ²:\", r2_score(y_test, y_pred))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23c38770",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import AdaBoostRegressor\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.metrics import r2_score\n",
    "\n",
    "# Base model\n",
    "base_tree = DecisionTreeRegressor(max_depth=3)\n",
    "\n",
    "# AdaBoost Regressor\n",
    "ada = AdaBoostRegressor(\n",
    "    estimator=base_tree,\n",
    "    n_estimators=100,\n",
    "    learning_rate=0.1,\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "ada.fit(X_train, y_train)\n",
    "\n",
    "y_pred = ada.predict(X_test)\n",
    "print(\"RÂ²:\", r2_score(y_test, y_pred))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "456fa7fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from xgboost import XGBRegressor\n",
    "from sklearn.metrics import r2_score\n",
    "from sklearn.model_selection import train_test_split\n",
    " \n",
    "x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.2)\n",
    "xg = XGBRegressor()\n",
    "xg.fit(x_train, y_train)\n",
    " \n",
    "y_pred_train = xg.predict(x_train)\n",
    "y_pred = xg.predict(x_test)\n",
    " \n",
    "r2 = r2_score(y_pred_train, y_train)\n",
    "r2_test = r2_score(y_pred, y_test)\n",
    " \n",
    "print(r2)\n",
    "print(r2_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "610f9acf",
   "metadata": {},
   "outputs": [],
   "source": [
    "from xgboost import XGBRegressor\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import r2_score\n",
    "\n",
    "# Define model\n",
    "xgb = XGBRegressor(objective=\"reg:squarederror\", random_state=42)\n",
    "\n",
    "# Define hyperparameter grid\n",
    "param_grid = {\n",
    "    \"n_estimators\": [100, 200, 300],        # number of boosting rounds (trees)\n",
    "    \"learning_rate\": [0.01, 0.05, 0.1],    # step size shrinkage\n",
    "    \"max_depth\": [3, 4, 5],                # depth of trees\n",
    "    \"subsample\": [0.8, 1.0],               # fraction of samples for training\n",
    "    \"colsample_bytree\": [0.8, 1.0],        # fraction of features per tree\n",
    "    \"min_child_weight\": [1, 3, 5],         # min sum of instance weight in a child\n",
    "    \"gamma\": [0, 0.1, 0.3]                 # min loss reduction for split\n",
    "}\n",
    "\n",
    "# GridSearchCV\n",
    "grid = GridSearchCV(\n",
    "    estimator=xgb,\n",
    "    param_grid=param_grid,\n",
    "    cv=5,\n",
    "    scoring=\"r2\",\n",
    "    n_jobs=-1,\n",
    "    verbose=2\n",
    ")\n",
    "\n",
    "# Fit\n",
    "grid.fit(X_train, y_train)\n",
    "\n",
    "# Best params and score\n",
    "print(\"Best Parameters:\", grid.best_params_)\n",
    "print(\"Best CV RÂ²:\", grid.best_score_)\n",
    "\n",
    "# Evaluate on test set\n",
    "y_pred = grid.predict(X_test)\n",
    "print(\"Test RÂ²:\", r2_score(y_test, y_pred))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0fd6f38",
   "metadata": {},
   "outputs": [],
   "source": [
    "from xgboost import XGBRegressor\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from sklearn.metrics import r2_score\n",
    "import numpy as np\n",
    "\n",
    "# 1. Define the model\n",
    "xgb = XGBRegressor(\n",
    "    objective=\"reg:squarederror\",\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "# 2. Define hyperparameter search space\n",
    "param_dist = {\n",
    "    \"n_estimators\": np.arange(100, 1000, 100),        # number of trees\n",
    "    \"max_depth\": np.arange(3, 15, 1),                # tree depth\n",
    "    \"learning_rate\": np.linspace(0.01, 0.3, 10),     # shrinkage\n",
    "    \"subsample\": np.linspace(0.5, 1.0, 6),           # row sampling\n",
    "    \"colsample_bytree\": np.linspace(0.5, 1.0, 6),    # feature sampling\n",
    "    \"gamma\": [0, 0.1, 0.2, 0.3, 0.4],                # min loss reduction\n",
    "    \"reg_alpha\": [0, 0.01, 0.1, 1],                  # L1 regularization\n",
    "    \"reg_lambda\": [0.1, 1, 10]                       # L2 regularization\n",
    "}\n",
    "\n",
    "# 3. RandomizedSearchCV\n",
    "random_search = RandomizedSearchCV(\n",
    "    estimator=xgb,\n",
    "    param_distributions=param_dist,\n",
    "    n_iter=30,            # number of random combinations to try\n",
    "    scoring=\"r2\",         # evaluation metric\n",
    "    cv=5,                 # 5-fold cross validation\n",
    "    verbose=2,\n",
    "    random_state=42,\n",
    "    n_jobs=-1             # run in parallel\n",
    ")\n",
    "\n",
    "# 4. Fit the model\n",
    "random_search.fit(X_train, y_train)\n",
    "\n",
    "# 5. Best parameters and score\n",
    "print(\"Best Parameters:\", random_search.best_params_)\n",
    "print(\"Best CV RÂ²:\", random_search.best_score_)\n",
    "\n",
    "# 6. Evaluate on test data\n",
    "y_pred = random_search.predict(X_test)\n",
    "print(\"Test RÂ²:\", r2_score(y_test, y_pred))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cc8a9e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "import xgboost as xgb\n",
    " \n",
    "param_grid = {\n",
    "    'max_depth': [3, 4, 5, 6],\n",
    "    'learning_rate': [0.01, 0.1, 0.2],\n",
    "    'n_estimators': [100, 200, 300]\n",
    "}\n",
    " \n",
    "xgb_model = xgb.XGBRegressor()\n",
    "grid_search = GridSearchCV(xgb_model, param_grid, cv=5, scoring='neg_mean_squared_error')\n",
    "grid_search.fit(X_train, y_train)\n",
    " \n",
    "y_predict_train_cv = grid_search.predict(X_train)\n",
    "y_predict_test_cv = grid_search.predict(X_test)\n",
    " \n",
    "print(\"r2 score of training data: \",r2_score(y_train, y_predict_train_cv))\n",
    "print(\"\\n\")\n",
    "print(\"r2 score for test data: \",r2_score(y_test, y_predict_test_cv))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f1606b1",
   "metadata": {},
   "source": [
    "### ADABOOST ALGO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e91d52ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import AdaBoostRegressor\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.metrics import r2_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.datasets import make_regression\n",
    "\n",
    "# Generate sample data\n",
    "X, y = make_regression(n_samples=1000, n_features=10, noise=0.2, random_state=42)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Simple AdaBoost with DecisionTree stumps\n",
    "ada = AdaBoostRegressor(\n",
    "    estimator=DecisionTreeRegressor(max_depth=1),\n",
    "    n_estimators=100,   # default = 50\n",
    "    learning_rate=1.0, # default = 1.0\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "ada.fit(X_train, y_train)\n",
    "\n",
    "y_pred = ada.predict(X_test)\n",
    "print(\"Simple AdaBoost RÂ²:\", r2_score(y_test, y_pred))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51590716",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "# Define base estimator\n",
    "base_est = DecisionTreeRegressor()\n",
    "\n",
    "# Define AdaBoost\n",
    "ada = AdaBoostRegressor(estimator=base_est, random_state=42)\n",
    "\n",
    "# Grid of hyperparameters\n",
    "param_grid = {\n",
    "    \"n_estimators\": [50, 100, 200],\n",
    "    \"learning_rate\": [0.01, 0.1, 0.5, 1],\n",
    "    \"estimator__max_depth\": [1, 2, 3]\n",
    "}\n",
    "\n",
    "# GridSearchCV\n",
    "grid = GridSearchCV(\n",
    "    estimator=ada,\n",
    "    param_grid=param_grid,\n",
    "    cv=5,\n",
    "    scoring=\"r2\",\n",
    "    n_jobs=-1\n",
    ")\n",
    "\n",
    "grid.fit(X_train, y_train)\n",
    "\n",
    "print(\"Best Parameters:\", grid.best_params_)\n",
    "print(\"Best CV Score:\", grid.best_score_)\n",
    "\n",
    "# Test performance\n",
    "y_pred = grid.predict(X_test)\n",
    "print(\"Test RÂ² (GridSearch):\", r2_score(y_test, y_pred))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35127d9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import datasets\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Load dataset (Iris for classification)\n",
    "iris = datasets.load_iris()\n",
    "X, y = iris.data, iris.target\n",
    "\n",
    "# Split data\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "\n",
    "# Train SVM without tuning (default parameters)\n",
    "svm_clf = SVC()\n",
    "svm_clf.fit(X_train, y_train)\n",
    "\n",
    "# Predict\n",
    "y_pred = svm_clf.predict(X_test)\n",
    "\n",
    "# Accuracy\n",
    "print(\"SVM (without tuning) Accuracy:\", accuracy_score(y_test, y_pred))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b56786ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "# Define parameter grid for SVM\n",
    "param_grid = {\n",
    "    'C': [0.1, 1, 10],\n",
    "    'kernel': ['linear', 'rbf', 'poly'],\n",
    "    'gamma': ['scale', 'auto']\n",
    "}\n",
    "\n",
    "# Grid Search\n",
    "grid_search = GridSearchCV(SVC(), param_grid, cv=5, scoring='accuracy')\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "# Best model\n",
    "best_svm = grid_search.best_estimator_\n",
    "y_pred_best = best_svm.predict(X_test)\n",
    "\n",
    "print(\"SVM (with tuning) Best Params:\", grid_search.best_params_)\n",
    "print(\"SVM (with tuning) Accuracy:\", accuracy_score(y_test, y_pred_best))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75cac8fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_diabetes\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "\n",
    "# Load dataset (Diabetes for regression)\n",
    "diabetes = load_diabetes()\n",
    "X, y = diabetes.data, diabetes.target\n",
    "\n",
    "# Split data\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "\n",
    "# Train SVR without tuning (default parameters)\n",
    "svr_reg = SVR()\n",
    "svr_reg.fit(X_train, y_train)\n",
    "\n",
    "# Predict\n",
    "y_pred = svr_reg.predict(X_test)\n",
    "\n",
    "# Evaluation\n",
    "print(\"SVR (without tuning) MSE:\", mean_squared_error(y_test, y_pred))\n",
    "print(\"SVR (without tuning) RÂ² Score:\", r2_score(y_test, y_pred))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1cae19d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define parameter grid for SVR\n",
    "param_grid = {\n",
    "    'C': [0.1, 1, 10, 100],\n",
    "    'kernel': ['linear', 'rbf', 'poly'],\n",
    "    'gamma': ['scale', 'auto'],\n",
    "    'epsilon': [0.1, 0.2, 0.5]\n",
    "}\n",
    "\n",
    "# Grid Search\n",
    "grid_search = GridSearchCV(SVR(), param_grid, cv=5, scoring='r2')\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "# Best model\n",
    "best_svr = grid_search.best_estimator_\n",
    "y_pred_best = best_svr.predict(X_test)\n",
    "\n",
    "print(\"SVR (with tuning) Best Params:\", grid_search.best_params_)\n",
    "print(\"SVR (with tuning) MSE:\", mean_squared_error(y_test, y_pred_best))\n",
    "print(\"SVR (with tuning) RÂ² Score:\", r2_score(y_test, y_pred_best))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7216216e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "import numpy as np\n",
    "\n",
    "# RandomizedSearchCV\n",
    "param_dist = {\n",
    "    \"n_estimators\": np.arange(50, 300, 50),\n",
    "    \"learning_rate\": np.linspace(0.01, 1.0, 10),\n",
    "    \"estimator__max_depth\": np.arange(1, 6)\n",
    "}\n",
    "\n",
    "random_search = RandomizedSearchCV(\n",
    "    estimator=ada,\n",
    "    param_distributions=param_dist,\n",
    "    n_iter=20,        # number of random trials\n",
    "    cv=5,\n",
    "    scoring=\"r2\",\n",
    "    random_state=42,\n",
    "    n_jobs=-1\n",
    ")\n",
    "\n",
    "random_search.fit(X_train, y_train)\n",
    "\n",
    "print(\"Best Parameters (RandomSearch):\", random_search.best_params_)\n",
    "print(\"Best CV Score:\", random_search.best_score_)\n",
    "\n",
    "# Test performance\n",
    "y_pred = random_search.predict(X_test)\n",
    "print(\"Test RÂ² (RandomSearch):\", r2_score(y_test, y_pred))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
